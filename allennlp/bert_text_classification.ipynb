{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    epochs=2,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / \"jigsaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10813f710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], id: str,\n",
    "                         labels: np.ndarray) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "        \n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"id\"], row[label_cols].values,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the spacy tokenizer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/30/2019 14:30:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=\"bert-base-uncased\",\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "# apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:00, 589.29it/s]\n",
      "251it [00:00, 747.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<allennlp.data.instance.Instance at 0x1a21ae4390>,\n",
       " <allennlp.data.instance.Instance at 0x1a21ae4dd8>,\n",
       " <allennlp.data.instance.Instance at 0x1a21aed2e8>,\n",
       " <allennlp.data.instance.Instance at 0x1a21af0b38>,\n",
       " <allennlp.data.instance.Instance at 0x1a21af53c8>,\n",
       " <allennlp.data.instance.Instance at 0x1a21af5ac8>,\n",
       " <allennlp.data.instance.Instance at 0x1a21af5dd8>,\n",
       " <allennlp.data.instance.Instance at 0x1a21af8a20>,\n",
       " <allennlp.data.instance.Instance at 0x1a21aff160>,\n",
       " <allennlp.data.instance.Instance at 0x1a21aff748>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [[UNK],\n",
       "  [UNK],\n",
       "  the,\n",
       "  edit,\n",
       "  ##s,\n",
       "  made,\n",
       "  under,\n",
       "  my,\n",
       "  user,\n",
       "  ##name,\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  were,\n",
       "  reverted,\n",
       "  ##?,\n",
       "  [UNK],\n",
       "  weren,\n",
       "  ##',\n",
       "  ##t,\n",
       "  van,\n",
       "  ##dal,\n",
       "  ##isms,\n",
       "  ##,,\n",
       "  just,\n",
       "  closure,\n",
       "  on,\n",
       "  some,\n",
       "  [UNK],\n",
       "  after,\n",
       "  [UNK],\n",
       "  voted,\n",
       "  at,\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  please,\n",
       "  don,\n",
       "  ##',\n",
       "  ##t,\n",
       "  remove,\n",
       "  the,\n",
       "  template,\n",
       "  from,\n",
       "  the,\n",
       "  talk,\n",
       "  page,\n",
       "  since,\n",
       "  [UNK],\n",
       "  retired,\n",
       "  now,\n",
       "  ##.,\n",
       "  ##8,\n",
       "  ##9,\n",
       "  ##.,\n",
       "  ##20,\n",
       "  ##5,\n",
       "  ##.,\n",
       "  ##38,\n",
       "  ##.,\n",
       "  ##27],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer at 0x1a2135b208>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/30/2019 14:30:59 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 267/267 [00:00<00:00, 53411.51it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds, max_vocab_size=config.max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator is responsible for batching the data and preparing it for input into the model. We'll use the BucketIterator that batches text sequences of smilar lengths together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell the iterator how to numericalize the text data. We do this by passing the vocabulary to the iterator. This step is easy to forget so be careful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[ 101,  100,  100,  ...,    0,    0,    0],\n",
       "          [ 101,  100,  100,  ..., 2001, 2130,  102],\n",
       "          [ 101, 3492, 2172,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 101, 1000,  100,  ..., 6414, 5275,  102],\n",
       "          [ 101,  100,  100,  ...,  100, 2017,  102],\n",
       "          [ 101,  100,  100,  ..., 4189, 2224,  102]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "          [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "          [ 1,  2,  3,  ..., 96, 97, 98]]),\n",
       "  'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " 'id': ['004b103182fb1eab',\n",
       "  '0034065c7b12a7a2',\n",
       "  '0015f4aa35ebe9b5',\n",
       "  '008f1b06428778fe',\n",
       "  '00882ab8cfa42274',\n",
       "  '001d874a4d3e8813',\n",
       "  '002c9cccf2f1d05b',\n",
       "  '00a981826dc7c952',\n",
       "  '00037261f536c51d',\n",
       "  '002746baedcdff10',\n",
       "  '00a6754c92b4af4f',\n",
       "  '00587c559177dcf2',\n",
       "  '0006f16e4e9f292e',\n",
       "  '0092f0871cc66dbc',\n",
       "  '0091aec11b57d12e',\n",
       "  '001cadfd324f8087',\n",
       "  '005306a4c109dab3',\n",
       "  '007db1f1477ea977',\n",
       "  '00480b6e1f19601b',\n",
       "  '005cec874506e9d9',\n",
       "  '005f59485fcddeb0',\n",
       "  '001d8e7be417776a',\n",
       "  '004b073d5b456b15',\n",
       "  '00078f8ce7eb276d',\n",
       "  '0038f191ffc93d75',\n",
       "  '006774d59329b7bd',\n",
       "  '00584d887401f47b',\n",
       "  '00a317acddff8a62',\n",
       "  '005e6b1369cbe377',\n",
       "  '00822d0d01752c7e',\n",
       "  '00218d74784ce50b',\n",
       "  '001810bf8c45bf5f',\n",
       "  '0060062dd4db5195',\n",
       "  '001363e1dbe91225',\n",
       "  '000c0dfd995809fa',\n",
       "  '0021fe88bc4da3e6',\n",
       "  '005eb6d31a8d821e',\n",
       "  '009820fe28cd24dc',\n",
       "  '0001b41b1c6bb37e',\n",
       "  '0048de0c9422f64f',\n",
       "  '00957fadc476d7d9',\n",
       "  '006854d70298693e',\n",
       "  '003a19c04c079bf7',\n",
       "  '006de7a80921e04b',\n",
       "  '0016e01b742b8da3',\n",
       "  '00328eadb85b3010',\n",
       "  '0009eaea3325de8c',\n",
       "  '009565ee1bc64e68',\n",
       "  '007e1e47cd0e2fec',\n",
       "  '004176f28a17bf45',\n",
       "  '00905910dcbcc8aa',\n",
       "  '000cfee90f50d471',\n",
       "  '004b975fabbbffa9',\n",
       "  '006a4cdda4960588',\n",
       "  '0005300084f90edc',\n",
       "  '00148d055a169b93',\n",
       "  '0082b5a7b4a67da2',\n",
       "  '001b2dd65d9d925c',\n",
       "  '008a1e9c45de8138',\n",
       "  '0033b9d5ccd499fb',\n",
       "  '0098257c6952c9e3',\n",
       "  '005ed4dfecd86188',\n",
       "  '004d07d94cb92e35',\n",
       "  '001ee16c46a99262'],\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  100,  100,  ...,    0,    0,    0],\n",
       "        [ 101,  100,  100,  ..., 2001, 2130,  102],\n",
       "        [ 101, 3492, 2172,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1000,  100,  ..., 6414, 5275,  102],\n",
       "        [ 101,  100,  100,  ...,  100, 2017,  102],\n",
       "        [ 101,  100,  100,  ..., 4189, 2224,  102]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=len(label_cols)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/30/2019 14:31:00 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "01/30/2019 14:31:00 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmpzzti7kvp\n",
      "01/30/2019 14:31:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "        top_layer_only=True, # conserve memory\n",
    ")\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                            # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                           allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "class BertSentencePooler(Seq2VecEncoder):\n",
    "    def forward(self, embs: torch.tensor, \n",
    "                mask: torch.tensor=None) -> torch.tensor:\n",
    "        # extract first token tensor\n",
    "        return embs[:, 0]\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return BERT_DIM\n",
    "    \n",
    "encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how simple and modular the code for initializing the model is. All the complexity is delegated to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[ 101,  100,  100,  ...,    0,    0,    0],\n",
       "         [ 101,  100,  100,  ..., 2001, 2130,  102],\n",
       "         [ 101, 3492, 2172,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1000,  100,  ..., 6414, 5275,  102],\n",
       "         [ 101,  100,  100,  ...,  100, 2017,  102],\n",
       "         [ 101,  100,  100,  ..., 4189, 2224,  102]]),\n",
       " 'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "         [ 1,  2,  3,  ..., 96, 97, 98],\n",
       "         [ 1,  2,  3,  ..., 96, 97, 98]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3781,  0.1085,  0.4758, -0.0077,  0.1949,  0.0161],\n",
       "        [ 0.3596,  0.2196,  0.2151, -0.0192,  0.0527, -0.0243],\n",
       "        [ 0.4311,  0.1409,  0.0376, -0.3367, -0.0769, -0.0157],\n",
       "        [ 0.4335,  0.3701,  0.3405, -0.1184, -0.0094, -0.1064],\n",
       "        [ 0.4700,  0.0739,  0.1653, -0.0565, -0.0297,  0.0323],\n",
       "        [ 0.4285, -0.0113,  0.1163, -0.0126, -0.1320, -0.1505],\n",
       "        [ 0.5624,  0.2789,  0.0805, -0.0140,  0.1285, -0.0224],\n",
       "        [ 0.5625,  0.1163,  0.2514,  0.0678,  0.2661, -0.0518],\n",
       "        [ 0.3955,  0.0895,  0.2486, -0.2068, -0.0766, -0.0138],\n",
       "        [ 0.3479,  0.1997,  0.0116, -0.0916, -0.0028, -0.0203],\n",
       "        [ 0.6592,  0.1787,  0.2949, -0.1487,  0.2863,  0.0260],\n",
       "        [ 0.2323,  0.2659,  0.0830, -0.1042, -0.0649, -0.1018],\n",
       "        [ 0.2686,  0.1543,  0.1628, -0.2537, -0.0060, -0.0945],\n",
       "        [ 0.3834,  0.2561,  0.0754, -0.2584,  0.0843, -0.2079],\n",
       "        [ 0.3827,  0.1597,  0.1498, -0.0688,  0.0793, -0.3707],\n",
       "        [ 0.2815,  0.2058,  0.1377, -0.1842, -0.0728, -0.1119],\n",
       "        [ 0.2754,  0.0171,  0.3949, -0.0424, -0.2879, -0.1424],\n",
       "        [ 0.5527,  0.1252,  0.2689,  0.0392,  0.1642, -0.2565],\n",
       "        [ 0.3695,  0.2222,  0.1928, -0.1446,  0.0254, -0.1110],\n",
       "        [ 0.5437,  0.3901,  0.1173, -0.0347, -0.0100, -0.0234],\n",
       "        [ 0.4312,  0.2016,  0.1789,  0.0804, -0.1205, -0.1439],\n",
       "        [ 0.3749,  0.2178,  0.2054, -0.2724, -0.0371, -0.1445],\n",
       "        [ 0.4265,  0.2033,  0.1751, -0.2003,  0.0627,  0.0359],\n",
       "        [ 0.4360,  0.0607,  0.2807, -0.2222,  0.0242, -0.0776],\n",
       "        [ 0.6694,  0.0935,  0.4223, -0.1218,  0.2289, -0.0592],\n",
       "        [ 0.1970,  0.1079, -0.0615, -0.1473,  0.0612, -0.1298],\n",
       "        [ 0.4437,  0.0634, -0.0067, -0.2333, -0.0892, -0.1133],\n",
       "        [ 0.5884,  0.2454,  0.4686, -0.1574,  0.2696, -0.0269],\n",
       "        [ 0.2857,  0.1595, -0.1004, -0.0769, -0.1731, -0.1132],\n",
       "        [ 0.5569,  0.1370,  0.3843, -0.0529,  0.0860, -0.0676],\n",
       "        [ 0.3006,  0.0975,  0.1604, -0.1015,  0.0037, -0.0941],\n",
       "        [ 0.4711,  0.2071,  0.3048,  0.1729, -0.0077, -0.0340],\n",
       "        [ 0.3826,  0.0923,  0.0746, -0.1876,  0.0385, -0.0777],\n",
       "        [ 0.5285,  0.1411,  0.3547, -0.0936,  0.1698, -0.1483],\n",
       "        [ 0.2734,  0.2239, -0.0394, -0.3487, -0.1022, -0.0160],\n",
       "        [ 0.2680,  0.0821,  0.0289, -0.4175, -0.1116, -0.0209],\n",
       "        [ 0.0779,  0.0239, -0.1329, -0.2644,  0.1664,  0.0084],\n",
       "        [ 0.2450,  0.0688,  0.1376, -0.0251, -0.1054, -0.0463],\n",
       "        [ 0.5684,  0.0589,  0.1383, -0.1015,  0.2546, -0.1894],\n",
       "        [ 0.4030,  0.1969,  0.3100, -0.1996,  0.0087, -0.2280],\n",
       "        [ 0.5505,  0.2987,  0.2930,  0.1051, -0.0266, -0.0683],\n",
       "        [ 0.3291,  0.1557,  0.0928, -0.2578,  0.0294,  0.0174],\n",
       "        [ 0.4229,  0.1835,  0.1675, -0.0639,  0.1502, -0.0723],\n",
       "        [ 0.3100,  0.1076,  0.0896, -0.3074,  0.0840, -0.0572],\n",
       "        [ 0.3662,  0.0371,  0.2268, -0.1556,  0.1328,  0.0643],\n",
       "        [ 0.2271,  0.1558,  0.1132, -0.1953,  0.0649, -0.1291],\n",
       "        [ 0.3547,  0.0762,  0.1424, -0.1482,  0.1540,  0.0071],\n",
       "        [ 0.2208,  0.0653,  0.3840, -0.1740,  0.2947,  0.0443],\n",
       "        [ 0.5552,  0.2019,  0.1284, -0.0730,  0.0335, -0.0144],\n",
       "        [ 0.2217,  0.1907,  0.1367, -0.0321,  0.0038, -0.3224],\n",
       "        [ 0.4484,  0.1282,  0.0378, -0.3034,  0.0057,  0.0053],\n",
       "        [ 0.5607,  0.1894,  0.1961, -0.0153, -0.0087, -0.1991],\n",
       "        [ 0.1796,  0.0910,  0.2097, -0.1470,  0.1589, -0.2567],\n",
       "        [ 0.2349,  0.0136,  0.1181,  0.0316, -0.0904,  0.1063],\n",
       "        [ 0.6185,  0.2531,  0.3166, -0.1340,  0.0239, -0.0095],\n",
       "        [ 0.3109,  0.0898,  0.0065, -0.2519,  0.0420, -0.2142],\n",
       "        [ 0.4565,  0.1671,  0.3343, -0.1164, -0.0440,  0.0879],\n",
       "        [ 0.4333,  0.1312,  0.1765, -0.1201, -0.0261, -0.1322],\n",
       "        [ 0.3944,  0.2318,  0.0916, -0.1619,  0.0226,  0.1315],\n",
       "        [ 0.6004,  0.0717,  0.2719, -0.2521,  0.2540, -0.1357],\n",
       "        [ 0.4666,  0.2695,  0.2037, -0.0739,  0.0208, -0.0366],\n",
       "        [ 0.5216,  0.2500,  0.0574,  0.0436, -0.1119,  0.0112],\n",
       "        [ 0.6434,  0.1114,  0.4597, -0.0877, -0.0831, -0.1397],\n",
       "        [ 0.3884,  0.1884,  0.1846, -0.2066,  0.0329, -0.0386]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[ 0.5553,  0.0279,  0.2647, -0.0261, -0.1336, -0.0631],\n",
       "         [ 0.2300,  0.2907,  0.3286, -0.0085, -0.1682, -0.1022],\n",
       "         [ 0.3542,  0.2422,  0.0602, -0.2695, -0.0081, -0.0589],\n",
       "         [ 0.5020,  0.1734,  0.4356,  0.0111,  0.3813, -0.2373],\n",
       "         [ 0.3623,  0.1063,  0.3194, -0.1453,  0.0744, -0.0130],\n",
       "         [ 0.3431,  0.0989,  0.0216, -0.1020, -0.1255,  0.0649],\n",
       "         [ 0.3838,  0.1365,  0.0379, -0.0341,  0.1253, -0.0776],\n",
       "         [ 0.4509,  0.1045,  0.2311, -0.1048,  0.0933, -0.0869],\n",
       "         [ 0.3045,  0.0623,  0.0757, -0.1947, -0.0396, -0.0552],\n",
       "         [ 0.4100,  0.2782,  0.1424, -0.2658,  0.0340, -0.1136],\n",
       "         [ 0.4522,  0.1980, -0.0256, -0.2547, -0.0491, -0.1085],\n",
       "         [ 0.3956,  0.0516,  0.1346, -0.0352, -0.1077, -0.0150],\n",
       "         [ 0.1833,  0.0885,  0.2918, -0.0740, -0.0278, -0.0750],\n",
       "         [ 0.2402,  0.3118,  0.0473, -0.1252,  0.0389, -0.1903],\n",
       "         [ 0.4211,  0.2523,  0.2994, -0.0343,  0.1355, -0.1534],\n",
       "         [ 0.2376,  0.1596,  0.0159, -0.3046, -0.2187, -0.0191],\n",
       "         [ 0.4020,  0.0377,  0.2087, -0.1941, -0.2041, -0.0923],\n",
       "         [ 0.4438,  0.0168,  0.3044, -0.0566, -0.0251, -0.1031],\n",
       "         [ 0.3525,  0.1573,  0.1493, -0.1995,  0.0732, -0.0063],\n",
       "         [ 0.5170,  0.2763,  0.0541, -0.0434, -0.0932, -0.0783],\n",
       "         [ 0.4521,  0.2289,  0.2134, -0.0408, -0.0594, -0.1491],\n",
       "         [ 0.3000,  0.1084,  0.1258, -0.2273,  0.0769,  0.0265],\n",
       "         [ 0.3416,  0.1194,  0.4324, -0.2615,  0.2770, -0.1248],\n",
       "         [ 0.5163, -0.0256,  0.1412, -0.1079,  0.0649, -0.0704],\n",
       "         [ 0.5821,  0.1841,  0.2345, -0.1097, -0.0179, -0.1112],\n",
       "         [ 0.1460,  0.0609,  0.0533,  0.0082, -0.0391, -0.0097],\n",
       "         [ 0.4533,  0.0677, -0.0582, -0.1582, -0.0533, -0.0831],\n",
       "         [ 0.5261,  0.2646,  0.2454, -0.1995,  0.0545, -0.1897],\n",
       "         [ 0.3875,  0.1667,  0.0245, -0.1059, -0.0458, -0.1216],\n",
       "         [ 0.4656,  0.1907, -0.0410, -0.1819, -0.1950, -0.0799],\n",
       "         [ 0.4028,  0.0338,  0.2895,  0.0133, -0.0511, -0.0825],\n",
       "         [ 0.5570,  0.3840,  0.2774,  0.1990, -0.2156,  0.0896],\n",
       "         [ 0.4126,  0.1546,  0.1094, -0.2288,  0.0268, -0.2063],\n",
       "         [ 0.5586,  0.1645,  0.1016, -0.2528,  0.3011, -0.2760],\n",
       "         [ 0.2977,  0.2151,  0.1056, -0.2997, -0.0334, -0.0010],\n",
       "         [ 0.4836,  0.1124,  0.3749, -0.2626,  0.1799,  0.2111],\n",
       "         [ 0.2862,  0.1184,  0.1510, -0.2956,  0.0312, -0.0091],\n",
       "         [ 0.3736,  0.0561,  0.2153, -0.1391,  0.0150, -0.2356],\n",
       "         [ 0.5254,  0.2011,  0.1251, -0.1537,  0.0927, -0.0910],\n",
       "         [ 0.5668,  0.0793,  0.5120, -0.1310,  0.2500,  0.0309],\n",
       "         [ 0.6119,  0.0050,  0.3208,  0.1002,  0.0471, -0.1137],\n",
       "         [ 0.3961,  0.2166,  0.1579, -0.2552,  0.0628, -0.0235],\n",
       "         [ 0.3695,  0.2257,  0.1152, -0.0821,  0.1298, -0.0534],\n",
       "         [ 0.3104,  0.1553,  0.0915, -0.2148,  0.0020,  0.1157],\n",
       "         [ 0.3268,  0.0807,  0.2426, -0.2139,  0.0316, -0.1169],\n",
       "         [ 0.2935, -0.0318,  0.1887, -0.1108, -0.0564, -0.1253],\n",
       "         [ 0.2203,  0.1875,  0.1516, -0.2577, -0.0514, -0.0450],\n",
       "         [ 0.2611,  0.2121,  0.1809, -0.1238,  0.0799,  0.0377],\n",
       "         [ 0.5383,  0.0658,  0.2128, -0.0558, -0.0953, -0.1614],\n",
       "         [ 0.2153,  0.2333,  0.1919, -0.0836,  0.0289, -0.0599],\n",
       "         [ 0.4329, -0.0096,  0.1959,  0.0029, -0.0257,  0.0169],\n",
       "         [ 0.4898,  0.2186,  0.0372, -0.1792, -0.0185, -0.1379],\n",
       "         [ 0.0714,  0.1137,  0.1290, -0.0598,  0.2091, -0.1920],\n",
       "         [ 0.1847,  0.0183, -0.0347,  0.1073, -0.1343,  0.1590],\n",
       "         [ 0.6783,  0.1453,  0.3260, -0.0124, -0.0838,  0.0241],\n",
       "         [ 0.4144,  0.1885,  0.2058, -0.1600,  0.0015, -0.1081],\n",
       "         [ 0.5119,  0.2657,  0.2374, -0.0520, -0.0077,  0.0887],\n",
       "         [ 0.3553,  0.1557,  0.2544, -0.1676,  0.1032,  0.2060],\n",
       "         [ 0.6280,  0.3455,  0.1577, -0.0615,  0.0553, -0.0313],\n",
       "         [ 0.4423,  0.2010,  0.2684, -0.2008,  0.1048, -0.1262],\n",
       "         [ 0.4531,  0.1490,  0.1116, -0.1155,  0.0736, -0.1058],\n",
       "         [ 0.5186,  0.0959,  0.0210, -0.0756, -0.1347, -0.0095],\n",
       "         [ 0.5141,  0.0560,  0.2102, -0.1268, -0.0266, -0.2517],\n",
       "         [ 0.4567,  0.1942,  0.2108, -0.1635, -0.0891, -0.0939]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'loss': tensor(0.7386, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7450, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    num_epochs=config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/30/2019 14:31:49 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/30/2019 14:31:49 - INFO - allennlp.training.trainer -   Epoch 0/1\n",
      "01/30/2019 14:31:50 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2061.193216\n",
      "01/30/2019 14:31:50 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.7018 ||: 100%|██████████| 5/5 [00:44<00:00,  7.12s/it]\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2061.193  |       N/A\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   loss          |     0.702  |       N/A\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:44\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:44\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   Epoch 1/1\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2122.256384\n",
      "01/30/2019 14:32:34 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.6106 ||: 100%|██████████| 5/5 [00:46<00:00, 10.32s/it]\n",
      "01/30/2019 14:33:20 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/30/2019 14:33:20 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2122.256  |       N/A\n",
      "01/30/2019 14:33:20 - INFO - allennlp.training.trainer -   loss          |     0.611  |       N/A\n",
      "01/30/2019 14:33:20 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:46\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
