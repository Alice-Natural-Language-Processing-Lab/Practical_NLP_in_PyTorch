{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,\n",
    "    epochs=2,\n",
    "    hidden_sz=64,\n",
    "    max_seq_len=100, # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / \"jigsaw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11393e690>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
    "\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], id: str,\n",
    "                         labels: np.ndarray) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "        \n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"id\"], row[label_cols].values,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the spacy tokenizer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 17:35:41 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "\n",
    "token_indexer = PretrainedBertIndexer(\n",
    "    pretrained_model=\"bert-base-uncased\",\n",
    "    max_pieces=config.max_seq_len,\n",
    "    do_lowercase=True,\n",
    " )\n",
    "# apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "def tokenizer(s: str):\n",
    "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [00:00, 671.05it/s]\n",
      "251it [00:00, 894.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<allennlp.data.instance.Instance at 0x1a2bd16550>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bd16f98>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc8f4a8>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc93cf8>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc98588>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc98c88>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc98f98>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bc9bbe0>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bca2320>,\n",
       " <allennlp.data.instance.Instance at 0x1a2bca2908>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [[UNK],\n",
       "  [UNK],\n",
       "  the,\n",
       "  edit,\n",
       "  ##s,\n",
       "  made,\n",
       "  under,\n",
       "  my,\n",
       "  user,\n",
       "  ##name,\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  were,\n",
       "  reverted,\n",
       "  ##?,\n",
       "  [UNK],\n",
       "  weren,\n",
       "  ##',\n",
       "  ##t,\n",
       "  van,\n",
       "  ##dal,\n",
       "  ##isms,\n",
       "  ##,,\n",
       "  just,\n",
       "  closure,\n",
       "  on,\n",
       "  some,\n",
       "  [UNK],\n",
       "  after,\n",
       "  [UNK],\n",
       "  voted,\n",
       "  at,\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  [UNK],\n",
       "  please,\n",
       "  don,\n",
       "  ##',\n",
       "  ##t,\n",
       "  remove,\n",
       "  the,\n",
       "  template,\n",
       "  from,\n",
       "  the,\n",
       "  talk,\n",
       "  page,\n",
       "  since,\n",
       "  [UNK],\n",
       "  retired,\n",
       "  now,\n",
       "  ##.,\n",
       "  ##8,\n",
       "  ##9,\n",
       "  ##.,\n",
       "  ##20,\n",
       "  ##5,\n",
       "  ##.,\n",
       "  ##38,\n",
       "  ##.,\n",
       "  ##27],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer at 0x1a2b504550>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to build the vocab: all that is handled by the token indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator is responsible for batching the data and preparing it for input into the model. We'll use the BucketIterator that batches text sequences of smilar lengths together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell the iterator how to numericalize the text data. We do this by passing the vocabulary to the iterator. This step is easy to forget so be careful! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,   100,  2005,  ...,     0,     0,     0],\n",
       "          [  101,   100,   100,  ...,     0,     0,     0],\n",
       "          [  101,  1045,  2069,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [  101,   100,   100,  ...,     0,     0,     0],\n",
       "          [  101,  1000,   100,  ...,     0,     0,     0],\n",
       "          [  101,   100,  3521,  ...,  3109, 29612,   102]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "          [ 1,  2,  3,  ..., 20, 21, 22]]),\n",
       "  'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " 'id': ['00a54f5b8d29a2b7',\n",
       "  '0023daf96917e0d0',\n",
       "  '008faa76dd3eb890',\n",
       "  '0037fe4f8f5cdcfb',\n",
       "  '000897889268bc93',\n",
       "  '005de39c51ef844a',\n",
       "  '00a656a81c6f0903',\n",
       "  '009a3333aa4ac011',\n",
       "  '00190820581d90ce',\n",
       "  '00908f946bd2fd05',\n",
       "  '00a00eb46f78b1d2',\n",
       "  '0002bcb3da6cb337',\n",
       "  '00a3073791a5feaa',\n",
       "  '006a4e493b506d1e',\n",
       "  '001ffdcc3e7fb49c',\n",
       "  '004d912a00f79c89',\n",
       "  '0036e50f42d0b679',\n",
       "  '008fdd0ca51cdadc',\n",
       "  '006eaaaca322e12d',\n",
       "  '009ce66bcd5de288',\n",
       "  '009d0fe5e7ee720a',\n",
       "  '004b97c80705a548',\n",
       "  '0057e30091cf3e81',\n",
       "  '006b888560bcdfcd',\n",
       "  '002b90cc8a94c76b',\n",
       "  '004af8a71399a4dc',\n",
       "  '005fb1983ff191e9',\n",
       "  '000bfd0867774845',\n",
       "  '0007e25b2121310b',\n",
       "  '0086998b34865f93',\n",
       "  '00040093b2687caa',\n",
       "  '003bd094feef5263',\n",
       "  '00472b8e2d38d1ea',\n",
       "  '00582dcd527c8d7d',\n",
       "  '006e87872c8b370c',\n",
       "  '00128363e367d703',\n",
       "  '0083ead5c8afc356',\n",
       "  '006a263a08b593c5',\n",
       "  '00a9d33dea92c45c',\n",
       "  '001325b8b20ea8aa',\n",
       "  '000c6a3f0cd3ba8e',\n",
       "  '009cb63b1bd5daf9',\n",
       "  '0083790fbfe5014d',\n",
       "  '00025465d4725e87',\n",
       "  '00397b264deba890',\n",
       "  '00054a5e18b50dd4',\n",
       "  '0091798f05a311af',\n",
       "  '002e2d3db2b597c4',\n",
       "  '007a5055c23cc5a6',\n",
       "  '004a789c03eda830',\n",
       "  '00229d44f41f3acb',\n",
       "  '0001d958c54c6e35',\n",
       "  '009df88828f523c4',\n",
       "  '007810bde7d6ebd4',\n",
       "  '0020fd96ed3b8c8b',\n",
       "  '0097dd5c29bf7a15',\n",
       "  '0040017ef6277334',\n",
       "  '00a8fa03349203fb',\n",
       "  '00637960a7ec3436',\n",
       "  '0036621e4c7e10b5',\n",
       "  '0010833a96e1f886',\n",
       "  '003217c3eb469ba9',\n",
       "  '00a1aabcab9d44a0',\n",
       "  '0020e7119b96eeeb'],\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   100,  2005,  ...,     0,     0,     0],\n",
       "        [  101,   100,   100,  ...,     0,     0,     0],\n",
       "        [  101,  1045,  2069,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   100,   100,  ...,     0,     0,     0],\n",
       "        [  101,  1000,   100,  ...,     0,     0,     0],\n",
       "        [  101,   100,  3521,  ...,  3109, 29612,   102]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=len(label_cols)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 17:35:44 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/03/2019 17:35:44 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmpwnhi0hjt\n",
      "02/03/2019 17:35:47 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "        top_layer_only=True, # conserve memory\n",
    ")\n",
    "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                            # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                           allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "class BertSentencePooler(Seq2VecEncoder):\n",
    "    def forward(self, embs: torch.tensor, \n",
    "                mask: torch.tensor=None) -> torch.tensor:\n",
    "        # extract first token tensor\n",
    "        return embs[:, 0]\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return BERT_DIM\n",
    "    \n",
    "encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how simple and modular the code for initializing the model is. All the complexity is delegated to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[  101,   100,  2005,  ...,     0,     0,     0],\n",
       "         [  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2069,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   100,   100,  ...,     0,     0,     0],\n",
       "         [  101,  1000,   100,  ...,     0,     0,     0],\n",
       "         [  101,   100,  3521,  ...,  3109, 29612,   102]]),\n",
       " 'tokens-offsets': tensor([[ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ...,  0,  0,  0],\n",
       "         [ 1,  2,  3,  ..., 20, 21, 22]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5654e-01,  1.8664e-01,  3.6654e-01,  3.6568e-01,  3.5104e-01,\n",
       "         -2.1512e-01],\n",
       "        [ 4.2962e-01, -2.8918e-02,  3.6842e-01, -2.0817e-01,  2.6102e-01,\n",
       "         -3.2780e-01],\n",
       "        [ 3.8703e-01, -8.1013e-02, -4.3039e-02,  8.7068e-02, -3.4778e-02,\n",
       "         -1.0288e-01],\n",
       "        [ 5.1402e-01,  1.7842e-01,  5.9583e-02, -1.6438e-01,  8.5872e-03,\n",
       "         -2.7806e-02],\n",
       "        [ 5.5920e-01,  3.8523e-01,  1.9661e-01, -2.7572e-02,  1.0058e-01,\n",
       "         -1.6167e-01],\n",
       "        [ 5.2509e-01,  1.6955e-01,  6.7285e-02,  1.0717e-01,  2.7909e-01,\n",
       "          3.8543e-02],\n",
       "        [ 5.6904e-01,  2.4338e-01, -6.4044e-03, -6.4930e-02, -1.3761e-02,\n",
       "         -2.2972e-01],\n",
       "        [ 5.6019e-01,  8.8073e-02,  2.9777e-02, -1.0398e-01,  9.6796e-02,\n",
       "         -2.6863e-01],\n",
       "        [ 5.3734e-01,  3.4953e-01,  1.8585e-02,  8.1381e-02,  1.0827e-01,\n",
       "         -1.2286e-01],\n",
       "        [ 1.1801e-01,  7.4278e-02, -8.8033e-02, -1.0108e-01,  4.5776e-02,\n",
       "         -1.5425e-01],\n",
       "        [ 5.1139e-01, -8.1566e-02,  5.8132e-01, -2.0935e-01,  5.2529e-01,\n",
       "          1.5579e-01],\n",
       "        [ 7.7228e-01,  3.1205e-01,  6.5106e-02,  9.3704e-02, -1.2888e-01,\n",
       "         -1.7581e-01],\n",
       "        [ 4.7388e-01,  1.4506e-01,  2.4041e-02, -1.2206e-01,  9.5029e-02,\n",
       "         -3.2749e-02],\n",
       "        [ 3.0964e-01,  1.1410e-01, -4.9479e-02, -1.1459e-01, -3.9946e-02,\n",
       "         -3.4755e-02],\n",
       "        [ 4.5631e-01,  3.1430e-01,  4.6891e-01, -2.9329e-03,  2.4573e-01,\n",
       "         -2.5509e-01],\n",
       "        [ 5.4342e-01,  1.6065e-01,  7.6752e-02,  3.6090e-03,  4.3468e-02,\n",
       "         -1.3064e-01],\n",
       "        [ 5.3778e-01,  1.9576e-01,  7.4897e-02, -2.0781e-01,  1.0545e-01,\n",
       "         -1.2267e-01],\n",
       "        [ 3.0386e-01,  1.9172e-01,  7.1697e-02, -4.1076e-02,  9.5659e-02,\n",
       "          1.2356e-02],\n",
       "        [ 4.3487e-01,  1.2161e-01, -4.4284e-02, -3.2057e-01,  7.4814e-02,\n",
       "          4.3588e-02],\n",
       "        [ 4.0135e-01,  1.5016e-01, -6.6554e-02, -2.6888e-01, -1.6487e-01,\n",
       "          9.5366e-02],\n",
       "        [ 3.9099e-01, -9.5329e-02,  4.1304e-03, -1.3525e-01,  2.2417e-02,\n",
       "         -5.5549e-02],\n",
       "        [ 4.9082e-01,  1.1760e-01,  4.9967e-02, -1.2142e-01,  4.1056e-02,\n",
       "         -5.9300e-02],\n",
       "        [ 3.7982e-01,  1.8469e-01,  5.5498e-02, -2.2941e-01,  1.1786e-01,\n",
       "          1.8300e-01],\n",
       "        [ 3.1618e-01,  1.2640e-01, -1.9624e-01, -1.9092e-01, -1.5289e-01,\n",
       "         -1.0924e-02],\n",
       "        [ 2.7709e-01,  1.9487e-01, -2.3918e-02, -1.7891e-01,  4.4338e-02,\n",
       "          8.9768e-02],\n",
       "        [ 3.6384e-01,  1.5793e-01, -7.1155e-02, -2.1278e-01, -4.3691e-02,\n",
       "          3.2233e-02],\n",
       "        [ 4.4829e-01,  1.3669e-01, -6.4487e-03, -1.9438e-01,  6.6516e-02,\n",
       "         -3.5402e-02],\n",
       "        [ 3.8886e-01,  1.1345e-01, -1.4245e-01, -2.5846e-01,  4.2606e-02,\n",
       "          6.1424e-02],\n",
       "        [ 4.8794e-01,  2.2837e-01,  1.5854e-02, -1.3449e-01,  6.2575e-02,\n",
       "         -7.0389e-02],\n",
       "        [ 2.8556e-01,  6.2395e-02, -5.5038e-02, -2.2549e-01, -1.0443e-01,\n",
       "          3.8281e-02],\n",
       "        [ 2.9974e-01,  3.1956e-02, -1.3951e-02, -2.5952e-01,  6.1283e-02,\n",
       "         -1.8205e-01],\n",
       "        [ 4.6865e-01,  9.2170e-02, -5.7021e-02, -2.2156e-01,  7.7843e-02,\n",
       "         -2.0954e-02],\n",
       "        [ 3.2848e-01,  1.4799e-01, -1.5041e-01, -2.0988e-01, -2.3166e-02,\n",
       "          1.0167e-01],\n",
       "        [ 2.5324e-01,  6.5430e-02,  9.1321e-02, -2.2031e-01, -1.1157e-01,\n",
       "         -8.0018e-02],\n",
       "        [ 1.6304e-01, -6.6828e-02,  7.0670e-02, -1.5019e-01, -3.7723e-01,\n",
       "          1.8283e-01],\n",
       "        [ 7.4051e-01,  1.7666e-01,  3.6702e-01, -1.1503e-02,  2.4946e-01,\n",
       "         -4.6620e-03],\n",
       "        [ 7.1093e-02,  1.6961e-03,  1.2434e-02, -2.6661e-01, -1.1102e-02,\n",
       "          2.7679e-02],\n",
       "        [ 5.6156e-01,  7.4847e-02,  2.5905e-01, -7.6054e-02,  3.4443e-01,\n",
       "         -7.8426e-02],\n",
       "        [ 5.1044e-01,  2.6672e-01,  2.0065e-01,  1.1217e-01,  2.5730e-01,\n",
       "         -1.6923e-01],\n",
       "        [ 3.3514e-01,  1.2384e-01,  7.5875e-02, -2.5281e-01, -9.7144e-02,\n",
       "         -1.3163e-01],\n",
       "        [ 7.8923e-01,  7.9884e-02,  1.6420e-01, -2.5685e-01,  1.7979e-01,\n",
       "         -1.2905e-01],\n",
       "        [ 3.3111e-01,  1.3711e-01,  1.3166e-01, -1.6605e-01,  1.7581e-01,\n",
       "         -6.4646e-02],\n",
       "        [ 3.8041e-01,  8.7576e-02, -7.2867e-02, -2.1466e-01, -2.2635e-02,\n",
       "         -1.7616e-02],\n",
       "        [ 3.5011e-01,  1.3225e-01, -5.7328e-02, -2.7544e-01,  1.4024e-02,\n",
       "         -2.1153e-02],\n",
       "        [ 3.9748e-01,  1.2365e-01, -3.9835e-02, -1.2611e-01, -9.9594e-02,\n",
       "         -1.7779e-02],\n",
       "        [ 4.4217e-01,  1.1733e-01,  1.6232e-01, -2.2160e-01, -8.0214e-03,\n",
       "          1.3103e-01],\n",
       "        [ 6.0145e-01,  2.3207e-01,  4.8039e-02,  6.4511e-02,  9.5825e-02,\n",
       "         -3.6190e-02],\n",
       "        [ 1.3170e-01,  1.0039e-01,  5.3867e-02, -2.5484e-01, -9.2111e-02,\n",
       "          2.8825e-02],\n",
       "        [ 3.4886e-01,  1.8151e-01, -7.1866e-02, -2.5308e-01,  3.2204e-02,\n",
       "         -7.5088e-02],\n",
       "        [ 2.1137e-01,  3.3340e-01,  3.5615e-01, -3.6430e-02,  1.2606e-01,\n",
       "         -1.5627e-01],\n",
       "        [ 4.0803e-01,  2.0909e-01,  1.9958e-01, -3.1120e-01,  3.2902e-02,\n",
       "         -7.5172e-02],\n",
       "        [ 3.2264e-01,  1.0107e-01,  7.1668e-02, -1.8107e-01, -1.0487e-01,\n",
       "         -6.1573e-02],\n",
       "        [ 1.9404e-01, -5.8981e-02,  6.9843e-02, -8.1473e-02, -2.7020e-02,\n",
       "          1.3732e-02],\n",
       "        [ 7.6007e-01,  1.0481e-02,  3.9320e-01, -6.2463e-02,  3.8794e-01,\n",
       "          3.2302e-02],\n",
       "        [ 3.4331e-01,  1.0701e-01, -4.9680e-02, -2.3795e-01, -2.8062e-02,\n",
       "         -1.4092e-01],\n",
       "        [ 3.8288e-01,  7.0384e-02, -4.6732e-02, -2.2411e-01,  1.2191e-03,\n",
       "          5.9205e-03],\n",
       "        [ 5.4946e-01,  2.3314e-01, -3.5092e-02, -7.1740e-02, -1.8890e-01,\n",
       "         -6.8279e-02],\n",
       "        [ 5.3518e-01,  3.3777e-01,  1.0936e-01, -6.5709e-02,  7.5146e-02,\n",
       "         -2.8373e-01],\n",
       "        [ 6.8289e-01,  1.4116e-01, -2.4610e-02,  1.7595e-01,  1.0914e-01,\n",
       "         -5.7994e-02],\n",
       "        [ 3.0012e-01,  2.3454e-01, -5.5020e-02, -2.8690e-01, -1.1957e-02,\n",
       "          4.6358e-02],\n",
       "        [ 4.9121e-01,  1.2994e-01, -6.2571e-02, -2.8596e-01,  6.3902e-02,\n",
       "          6.0251e-02],\n",
       "        [ 4.2539e-01,  2.3713e-02,  6.8544e-02, -1.7572e-01, -4.3780e-05,\n",
       "         -1.0318e-01],\n",
       "        [ 2.6503e-01,  6.6297e-02, -1.0435e-01, -2.0409e-01, -5.2313e-02,\n",
       "          5.5941e-02],\n",
       "        [ 5.2959e-01,  1.0880e-02,  6.5538e-02, -1.6786e-01, -1.1773e-01,\n",
       "          1.7257e-01]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[ 4.6100e-01,  1.7326e-01,  2.2254e-01,  1.1410e-01,  2.4407e-01,\n",
       "          -1.5805e-01],\n",
       "         [ 6.3738e-01,  2.2056e-01,  2.9089e-02, -2.3310e-02,  1.3608e-01,\n",
       "          -2.5500e-01],\n",
       "         [ 2.8956e-01,  5.2109e-02, -5.5303e-02, -1.6900e-01, -1.0136e-01,\n",
       "           9.3308e-02],\n",
       "         [ 1.7099e-01,  1.3054e-01, -9.5852e-02, -2.4803e-01, -4.0757e-02,\n",
       "          -1.8549e-01],\n",
       "         [ 7.5998e-01,  2.4915e-01,  3.0788e-01,  1.6619e-01,  2.8302e-01,\n",
       "           1.4241e-02],\n",
       "         [ 4.6951e-01,  1.4172e-01, -3.9608e-02, -2.2060e-01, -1.5100e-01,\n",
       "           4.1976e-03],\n",
       "         [ 4.6982e-01,  2.5360e-01, -3.1616e-02, -2.1575e-01,  2.8362e-02,\n",
       "           1.2691e-01],\n",
       "         [ 5.4504e-01,  1.3360e-01,  2.0855e-01, -9.0100e-02,  2.3306e-01,\n",
       "          -3.2395e-01],\n",
       "         [ 5.2233e-01,  2.6013e-01,  9.8824e-02,  1.0170e-01,  2.2584e-01,\n",
       "          -7.9434e-04],\n",
       "         [ 2.9987e-01,  6.7261e-02, -4.3380e-02, -1.8769e-01, -2.1650e-01,\n",
       "           1.3009e-01],\n",
       "         [ 3.5877e-01, -3.5344e-02,  3.1710e-01, -1.1303e-01,  2.3747e-01,\n",
       "           7.8915e-02],\n",
       "         [ 7.1510e-01,  2.8682e-01, -2.8774e-02, -3.1799e-02, -4.1913e-02,\n",
       "          -1.7405e-01],\n",
       "         [ 3.9038e-01,  2.0478e-01, -1.7099e-01, -1.7351e-01,  2.4116e-01,\n",
       "          -1.8581e-01],\n",
       "         [ 5.0845e-01,  1.5384e-01,  3.9708e-01, -1.6536e-02,  3.5202e-01,\n",
       "           5.9074e-02],\n",
       "         [ 3.9720e-01,  1.7494e-01,  2.6842e-02, -1.6342e-01, -1.6910e-02,\n",
       "          -2.7258e-01],\n",
       "         [ 6.5630e-01,  3.7096e-01,  1.6240e-01, -5.3649e-03,  9.3735e-02,\n",
       "          -1.1428e-01],\n",
       "         [ 6.1125e-01,  2.8957e-01,  1.9387e-01, -8.5461e-02,  9.9892e-02,\n",
       "          -5.1931e-04],\n",
       "         [ 3.3987e-01,  9.7152e-02, -6.5040e-02, -2.9755e-01,  7.7682e-02,\n",
       "          -1.0497e-01],\n",
       "         [ 4.2072e-01,  1.7366e-01, -1.8387e-02, -3.0627e-01,  3.3474e-02,\n",
       "           6.3107e-02],\n",
       "         [ 3.5382e-01,  8.6764e-02, -1.4632e-01, -2.4638e-01, -3.4956e-03,\n",
       "           5.7588e-02],\n",
       "         [ 5.0610e-01,  3.6255e-03, -1.4808e-02, -1.8790e-01,  1.6383e-01,\n",
       "          -1.6602e-01],\n",
       "         [ 6.8226e-01,  3.6863e-02,  1.7275e-01, -1.9728e-01, -1.1036e-01,\n",
       "          -1.5059e-01],\n",
       "         [ 5.6961e-01,  1.4665e-01,  4.6543e-01,  8.2751e-02,  5.3028e-01,\n",
       "           3.1574e-01],\n",
       "         [ 3.2278e-01,  7.4282e-02, -2.1974e-02, -2.0212e-01, -3.8975e-02,\n",
       "           6.4558e-02],\n",
       "         [ 3.7741e-01,  2.1519e-01, -1.3625e-01, -1.9640e-01, -4.2927e-02,\n",
       "          -6.7300e-02],\n",
       "         [ 3.1856e-01,  9.8140e-02, -7.6977e-02, -2.4130e-01, -4.2346e-02,\n",
       "           6.4810e-02],\n",
       "         [ 4.8428e-01,  1.0738e-01,  1.9653e-01, -1.4359e-01,  5.3839e-02,\n",
       "          -1.1557e-01],\n",
       "         [ 4.0527e-01,  6.2649e-02, -1.5361e-02, -2.7368e-01, -1.4002e-02,\n",
       "           6.2581e-02],\n",
       "         [ 4.8176e-01,  1.1724e-01, -9.5728e-02, -2.2550e-01,  4.7227e-02,\n",
       "          -2.5176e-01],\n",
       "         [ 2.6599e-01,  5.9284e-02, -3.5917e-02, -2.3860e-01, -5.5151e-02,\n",
       "           5.8960e-03],\n",
       "         [ 5.0603e-01,  1.7707e-01,  2.9783e-01,  1.3493e-01,  9.4340e-02,\n",
       "          -1.7223e-02],\n",
       "         [ 5.7428e-01,  7.7317e-02,  1.6082e-01, -1.0725e-01,  1.6174e-01,\n",
       "          -7.5396e-02],\n",
       "         [ 4.3900e-01,  2.1765e-01, -1.1197e-01, -2.3294e-01, -9.0713e-02,\n",
       "           3.9299e-02],\n",
       "         [ 3.8429e-01,  8.9694e-02,  4.9582e-02, -2.2651e-01, -4.4198e-02,\n",
       "           6.8990e-03],\n",
       "         [ 1.6545e-01,  6.6355e-02, -3.0624e-02, -6.0183e-02, -1.9868e-01,\n",
       "           1.8627e-01],\n",
       "         [ 8.0334e-01,  2.9775e-01,  5.1920e-01, -1.6648e-01,  3.7555e-01,\n",
       "          -6.5449e-02],\n",
       "         [ 2.1111e-01,  7.9290e-02, -7.0801e-02, -3.1272e-01,  1.7001e-04,\n",
       "           3.5456e-02],\n",
       "         [ 4.8035e-01,  2.2677e-01,  1.6261e-01, -3.4098e-02,  9.5839e-02,\n",
       "          -2.1160e-01],\n",
       "         [ 5.2713e-01,  1.1872e-01,  1.3707e-01, -2.0073e-01,  3.1186e-01,\n",
       "          -1.8080e-01],\n",
       "         [ 4.5489e-01,  1.5139e-01,  8.5526e-02, -2.3973e-01, -4.8604e-02,\n",
       "          -1.1268e-01],\n",
       "         [ 8.5045e-01,  3.2233e-01,  1.9422e-01, -8.3220e-02,  2.5231e-01,\n",
       "          -3.5597e-02],\n",
       "         [ 3.4391e-01,  2.7642e-01,  2.8730e-01,  1.0605e-02,  1.6011e-02,\n",
       "          -9.0026e-02],\n",
       "         [ 2.6342e-01,  8.3767e-02,  5.1299e-02, -4.3478e-02, -1.1535e-01,\n",
       "          -1.1888e-01],\n",
       "         [ 4.2723e-01,  2.4827e-01,  1.1355e-02, -2.4940e-01,  9.7826e-03,\n",
       "           3.5164e-02],\n",
       "         [ 4.2118e-01,  1.9312e-01, -1.3078e-01,  7.1369e-02, -9.2896e-02,\n",
       "          -7.8959e-02],\n",
       "         [ 4.2933e-01, -2.0291e-02,  4.3849e-01, -2.1664e-01,  3.0380e-01,\n",
       "           2.7201e-01],\n",
       "         [ 6.8639e-01,  1.6184e-01,  1.1045e-01,  1.4509e-01,  1.2982e-01,\n",
       "          -1.2643e-01],\n",
       "         [ 2.9321e-01,  7.3807e-02,  4.8950e-02, -2.0987e-01, -5.8449e-02,\n",
       "           1.5801e-02],\n",
       "         [ 2.8903e-01,  1.3771e-01,  2.6412e-02, -2.5877e-01,  5.3739e-02,\n",
       "          -6.3946e-02],\n",
       "         [ 1.8477e-01,  4.1254e-01,  2.5969e-01, -1.0680e-01,  5.9721e-02,\n",
       "          -4.0184e-02],\n",
       "         [ 5.1429e-01,  1.6536e-01,  3.5918e-01, -1.4190e-02,  2.0700e-01,\n",
       "          -1.3733e-01],\n",
       "         [ 3.5149e-01,  1.0319e-01,  2.7621e-03, -3.1206e-01, -5.1395e-02,\n",
       "          -2.8856e-03],\n",
       "         [ 3.6449e-01,  1.1521e-01,  1.1373e-01, -2.3831e-01, -1.4470e-01,\n",
       "          -1.8663e-01],\n",
       "         [ 5.0011e-01,  1.2959e-02, -8.7953e-02, -3.1519e-03,  7.8721e-02,\n",
       "          -2.1522e-01],\n",
       "         [ 4.5306e-01,  1.3111e-01,  7.9472e-02, -2.5846e-01,  4.9805e-03,\n",
       "          -3.4763e-01],\n",
       "         [ 3.5380e-01,  8.7103e-02, -2.1124e-02, -2.2347e-01,  9.7642e-02,\n",
       "           4.8928e-02],\n",
       "         [ 5.9905e-01,  1.7917e-01,  1.0308e-02, -6.2625e-02, -2.0742e-01,\n",
       "          -1.5287e-01],\n",
       "         [ 6.2718e-01,  1.6813e-01, -4.1449e-02, -1.3881e-01, -1.3098e-01,\n",
       "          -3.8197e-01],\n",
       "         [ 7.0090e-01, -2.9271e-02,  1.0887e-01,  1.0868e-01,  3.9921e-01,\n",
       "           6.1317e-02],\n",
       "         [ 2.5520e-01,  5.3549e-02, -1.3536e-02, -2.0437e-01, -1.5347e-01,\n",
       "           2.4739e-02],\n",
       "         [ 5.5360e-01,  9.7337e-02, -2.7666e-02, -2.7328e-01, -2.9974e-01,\n",
       "          -8.4187e-02],\n",
       "         [ 3.6012e-01,  9.6099e-02,  1.2685e-02, -2.5735e-01, -3.2776e-02,\n",
       "           3.1218e-03],\n",
       "         [ 3.8247e-01,  2.1893e-02, -6.5536e-03, -1.6346e-01,  2.2768e-03,\n",
       "           1.3068e-02],\n",
       "         [ 6.0272e-01,  7.6376e-02,  2.8850e-01, -1.1569e-01,  1.2049e-01,\n",
       "           2.6532e-01]], grad_fn=<AddmmBackward>),\n",
       " 'loss': tensor(0.7314, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7290, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    num_epochs=config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/03/2019 17:35:58 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "02/03/2019 17:35:58 - INFO - allennlp.training.trainer -   Epoch 0/1\n",
      "02/03/2019 17:35:58 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1287.852032\n",
      "02/03/2019 17:35:58 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.6990 ||: 100%|██████████| 5/5 [00:33<00:00,  6.84s/it]\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   loss          |     0.699  |       N/A\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1287.852  |       N/A\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:33\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:33\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   Epoch 1/1\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2005.065728\n",
      "02/03/2019 17:36:31 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.6103 ||: 100%|██████████| 5/5 [00:31<00:00,  6.84s/it]\n",
      "02/03/2019 17:37:03 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "02/03/2019 17:37:03 - INFO - allennlp.training.trainer -   loss          |     0.610  |       N/A\n",
      "02/03/2019 17:37:03 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2005.066  |       N/A\n",
      "02/03/2019 17:37:03 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:31\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
